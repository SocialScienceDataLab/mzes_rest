# RESTful

* interoperability between computer systems on the Internet. 
* requesting systems to access and manipulate textual representations of Web resources 

Example:

https://api.ipify.org/?format=json

> curl -s 'https://api.ipify.org/?format=json'

```{r}
require(httr)
require(jsonlite)
require(magrittr)
require(tidyverse)
require(SocialMediaLab)
require(magrittr)
require(igraph)
require(tuber)
require(Rfacebook)
require(lubridate)
require(magrittr)
require(igraph)
require(plotly)


res <- GET("https://api.ipify.org/", query = list(format = 'json'))
content(res, type = "text", encoding = 'utf-8')
content(res, type = "text", encoding = 'utf-8') %>% fromJSON

content(res, type = "text", encoding = 'utf-8') %>% fromJSON %>% `[[`("ip")
```

More sophisticated example

API article: https://github.com/derhuerst/db-rest/blob/master/docs/index.md

https://1.db.transport.rest/stations?query=mannheim


```{r}
res <- GET("https://1.db.transport.rest/stations", query = list(query = 'mannheim'))
mannheim_sid <- content(res)[[1]]$id
res <- GET("https://1.db.transport.rest/stations", query = list(query = 'berlin'))
berlin_sid <- content(res)[[1]]$id

##https://1.db.transport.rest/journeys?from=8000244&to=8011160
res <- GET("https://1.db.transport.rest/journeys", query = list(from = mannheim_sid, to = berlin_sid))
```

# HTTP Methods

* GET
Regular web request from browser, query strings (parameters) encoded in the URL

* POST
Request with information / file, query not encoded in the URL

* DELETE / HEAD / PUT / ...

Example: https://developers.facebook.com/docs/graph-api/reference/page

# Facebook

https://graph.facebook.com/AngelaMerkel

Access token: your id, determining your right and measuring your usage.

https://developers.facebook.com/tools/explorer

```{r}
res <- GET("https://graph.facebook.com/AngelaMerkel")
content(res)
temptoken <- ""
res <- GET("https://graph.facebook.com/", path = "AngelaMerkel", query = list(access_token = temptoken))
content(res)
res <- GET("https://graph.facebook.com/", path = "AngelaMerkel/likes", query = list(access_token = temptoken))
res <- GET("https://graph.facebook.com/", path = "AngelaMerkel/feed", query = list(access_token = temptoken))
content(res)$data[[1]]$id
res <- GET("https://graph.facebook.com/", path = paste0(content(res)$data[[1]]$id, "/", "comments"), query = list(access_token = temptoken, fields = "from"))

### What POST does.

```{r}
### NEED TO HAVE USER PUBLISH_ACTION PERMISSION.

token2 <- ""
res <- POST("https://graph.facebook.com/", path = "me/feed", query = list(access_token = token2, message = "Ich kann die US-Wahlen beeinflussen."))
```

Tedious!

## Abstractions

Rfacebook by Pablo BarberÃ¡ (LSE).

```{r}
## devtools::install_github("pablobarbera/Rfacebook/Rfacebook")
AM <- getPage("AngelaMerkel", feed = TRUE, reactions = TRUE, token = temptoken, api = 'v2.8')
post <- getPost(AM$id[1], token = temptoken, n = 1000, likes = TRUE, comments = TRUE)
post$comments[1,]
```

## Longterm token

Facebook authentication

https://developers.facebook.com/apps

* add product: login
* add OA redirect URIs
* add site URL
* add App Domains

```{r}
app_id <- ""
app_secret <- ""
longtoken <- fbOAuth(app_id = app_id, app_secret = app_secret)
SPDposts <- getPage(page = "SPD", token = longtoken, n = 100, feed = TRUE)
post <- getPost(AM$id[1], token = longtoken, n = 1000, likes = TRUE, comments = TRUE)
post$comments[1,]
```

## SocialMediaLab

An R project created at The Virtual Observatory for the Study of Online Networks (VOSON) Lab at ANU by Tim Graham and Robert Ackland.

I contributed the ACC workflow to simpify the usage.

* Authenticate - API related authentication
* Collect - Data collection from social media APIs
* Create - Create networks from social media data

http://vosonlab.net/SocialMediaLab/access_API

### Facebook

SML's Facebook data collection works only at the page level. The latest update of FB API (v.2.10) breaks it and no longer possible to collect data about users. But we can still get data about pages comment on another page.

It can only generate Bimodal (two-mode) network of commenters and posts.

```{r}
## devtools::install_github('vosonlab/SocialMediaLab/SocialMediaLab')


Authenticate("facebook", appID = app_id, appSecret = app_secret, extendedPermissions = TRUE) %>% SaveCredential()
LoadCredential() %>% Collect(pageName = "SPD", rangeFrom = "2017-09-25", rangeTo = '2018-02-12') -> SPDdata
SPDdata %>% Create('Bimodal') -> SPDbimodal

### You can do it in a single pipeline
Authenticate("facebook", appID = app_id, appSecret = app_secret, extendedPermissions = TRUE) %>% Collect(pageName = "SPD", rangeFrom = "2017-09-25", rangeTo = '2018-02-12') %>% Create('Bimodal') -> SPDbimodal

plot(SPDbimodal, vertex.label = V(SPDbimodal)$username, vertex.color = ifelse(V(SPDbimodal)$type == "User", "Red", "White"))
SPD_commenters <- data.frame(username = V(SPDbimodal)$username[match(names(sort(degree(SPDbimodal, mode = 'out'), TRUE)), V(SPDbimodal)$label)], degree = as.vector(sort(degree(SPDbimodal, mode = 'out'), TRUE)))
SPD_commenters %>% filter(degree > 0)
```

### Twitter

https://developer.twitter.com/en/docs/api-reference-index
https://apps.twitter.com/

SML's Twitter data collection is based on Keyword search. It can generate:

1. Bimodal network
2. Semantic network of tweets
3. Actor network of mentions.

NB: Twitter API can only search for 2 weeks of tweets, with the maximum of 1,500 tweets per query. You may need continuous surveillance. If you need historical data, you need to purchase it from GNIP.

```{r}

## beware of copy-and-pasting, remember to check for extra spaces.


tw_apikey <- ""
tw_apisecret <- ""
tw_accesstoken <- ""
tw_accesstokensecret <- ""

Authenticate("twitter", apiKey=tw_apikey, apiSecret=tw_apisecret, accessToken=tw_accesstoken, accessTokenSecret=tw_accesstokensecret) %>% Collect(searchTerm="@alice_weidel") %>% Create("Semantic") -> alice_semnet

betweenness(alice_semnet) %>% sort
degree(alice_semnet) %>% sort


Authenticate("twitter", apiKey=tw_apikey, apiSecret=tw_apisecret, accessToken=tw_accesstoken, accessTokenSecret=tw_accesstokensecret) %>% Collect(searchTerm="@alice_weidel") %>% Create("Actor") -> alice_actor


betweenness(alice_actor) %>% sort

Authenticate("twitter", apiKey=tw_apikey, apiSecret=tw_apisecret, accessToken=tw_accesstoken, accessTokenSecret=tw_accesstokensecret) %>% Collect(searchTerm="@realdonaldtrump") %>% Create("Actor") -> donald_actor

(alice_actor %u% donald_actor) %>% plot

## accumlation of data

Authenticate("twitter", apiKey=tw_apikey, apiSecret=tw_apisecret, accessToken=tw_accesstoken, accessTokenSecret=tw_accesstokensecret) %>% Collect(searchTerm="#rstats") -> r_data

### later

Authenticate("twitter", apiKey=tw_apikey, apiSecret=tw_apisecret, accessToken=tw_accesstoken, accessTokenSecret=tw_accesstokensecret) %>% Collect(searchTerm="#rstats") -> r_data2

r_data$id

r_data %>% bind_rows(r_data2) %>% distinct(id, .keep_all = TRUE) -> r_data

nrow(r_data)

r_data %>% Create("Actor") %>% betweenness %>% sort

# You may even do this

Authenticate("twitter", apiKey=tw_apikey, apiSecret=tw_apisecret, accessToken=tw_accesstoken, accessTokenSecret=tw_accesstokensecret) %>% Collect(searchTerm="#rstats") -> r_data

while(TRUE) {
    Sys.sleep(600) # every ten mins
    Authenticate("twitter", apiKey=tw_apikey, apiSecret=tw_apisecret, accessToken=tw_accesstoken, accessTokenSecret=tw_accesstokensecret) %>% Collect(searchTerm="#rstats") -> r_data2
    r_data %>% bind_rows(r_data2) %>% distinct(id, .keep_all = TRUE) -> r_data
}

```


### Youtube (Weak)

Google API console

https://console.developers.google.com/cloud-resource-manager

NB: only get first 100 comments

```{r}
Authenticate("youtube", apiKey = "") %>% Collect(videoIDs = c("6Ejga4kJUts", "hUFPooqKllA", "Yam5uK6e-bQ", "95HqlWRFrAk"), verbose = TRUE) -> yt_data
```
```{r}

yt_oauth(app_id = "", app_secret = "")
cranberries <- get_all_comments(video_id = 'hxsJvKYyVyg')

ymd_hms(as.character(cranberries$publishedAt)) %>% floor_date(unit = "day") %>% data_frame(date = .) %>% group_by(date) %>% tally %>% ungroup -> cran_date

range(cran_date$date)

res <- data_frame(date = seq(from = min(cran_date$date), to = max(cran_date$date), by = 'day'))

res %>% left_join(cran_date, by = "date") %>% replace_na(list(n = 0)) %>% ggplot(aes(x = date, y = n)) + geom_line()
```

https://www.youtube.com/watch?v=yXQViqx6GMY

```{r}

christmas <- get_all_comments(video_id = 'yXQViqx6GMY')
christmas$publishedAt <- ymd_hms(as.character(christmas$publishedAt))

saveRDS(christmas, "christmas.RDS")

christmas <- readRDS('christmas.RDS')
ymd_hms(as.character(christmas$publishedAt)) %>% floor_date(unit = "day") %>% data_frame(date = .) %>% group_by(date) %>% tally %>% ungroup -> cran_date

range(cran_date$date)

res <- data_frame(date = seq(from = min(cran_date$date), to = max(cran_date$date), by = 'day'))

res %>% left_join(cran_date, by = "date") %>% replace_na(list(n = 0)) -> res

ggplot(res, aes(x = date, y = n)) + geom_line() -> gg
ggplotly(gg)

res %>% filter(date >= as.Date('2017-05-01') & date <= as.Date('2017-07-31')) %>% ggplot(aes(x = date, y = n)) + geom_line()

```

https://www.reddit.com/r/me_irl/comments/6en7cy/meirl/?st=jdpzotmd&sh=9c033d72

```{r}
### text analysis: super simple one. Please go to the QTA workshop.
require(quanteda)
christmas <- readRDS('christmas.RDS')
christmas_c <- corpus(x = as.character(christmas$textOriginal), docvars = christmas)
pub2017 <- (as.Date(as.character(christmas$publishedAt)) < as.Date('2017-01-01'))
christmas_dfm <- dfm(christmas_c, remove = stopwords('english'), remove_punct = TRUE, remove_url = TRUE)
textplot_keyness(textstat_keyness(christmas_dfm, pub2017))
```

### Weibo
